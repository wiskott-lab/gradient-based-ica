{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlynur/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boo\n",
      "resetting env. episode reward total was -20.0. running mean: -20.0\n",
      "ep1: game finished, reward: -1.0\n",
      "14.484034999999995\n",
      "resetting env. episode reward total was -17.0. running mean: -19.970000000000002\n",
      "ep2: game finished, reward: -1.0\n",
      "20.361564\n",
      "resetting env. episode reward total was -15.0. running mean: -19.9203\n",
      "ep3: game finished, reward: -1.0\n",
      "17.236117\n",
      "resetting env. episode reward total was -18.0. running mean: -19.901097\n",
      "ep4: game finished, reward: -1.0\n",
      "14.998988000000011\n",
      "resetting env. episode reward total was -19.0. running mean: -19.89208603\n",
      "ep5: game finished, reward: -1.0\n",
      "16.269700999999998\n",
      "resetting env. episode reward total was -16.0. running mean: -19.853165169700002\n",
      "ep6: game finished, reward: -1.0\n",
      "15.105128999999991\n",
      "resetting env. episode reward total was -18.0. running mean: -19.834633518003002\n",
      "ep7: game finished, reward: -1.0\n",
      "18.931928999999997\n",
      "resetting env. episode reward total was -15.0. running mean: -19.78628718282297\n",
      "ep8: game finished, reward: -1.0\n",
      "17.452203999999995\n",
      "resetting env. episode reward total was -18.0. running mean: -19.76842431099474\n",
      "ep9: game finished, reward: -1.0\n",
      "15.36782100000002\n",
      "resetting env. episode reward total was -21.0. running mean: -19.780740067884793\n",
      "ep10: game finished, reward: -1.0\n",
      "19.98554199999998\n",
      "resetting env. episode reward total was -17.0. running mean: -19.752932667205947\n",
      "ep11: game finished, reward: -1.0\n",
      "18.000545999999986\n",
      "resetting env. episode reward total was -17.0. running mean: -19.725403340533887\n",
      "ep12: game finished, reward: -1.0\n",
      "19.87965699999998\n",
      "resetting env. episode reward total was -17.0. running mean: -19.69814930712855\n",
      "ep13: game finished, reward: -1.0\n",
      "16.000884000000013\n",
      "resetting env. episode reward total was -19.0. running mean: -19.691167814057266\n",
      "ep14: game finished, reward: -1.0\n",
      "15.206512000000032\n",
      "resetting env. episode reward total was -20.0. running mean: -19.694256135916692\n",
      "ep15: game finished, reward: -1.0\n",
      "19.326733999999988\n",
      "resetting env. episode reward total was -19.0. running mean: -19.68731357455753\n",
      "ep16: game finished, reward: -1.0\n",
      "26.10797100000002\n",
      "resetting env. episode reward total was -15.0. running mean: -19.64044043881195\n",
      "ep17: game finished, reward: -1.0\n",
      "19.78216900000001\n",
      "resetting env. episode reward total was -16.0. running mean: -19.60403603442383\n",
      "ep18: game finished, reward: -1.0\n",
      "18.37302600000004\n",
      "resetting env. episode reward total was -19.0. running mean: -19.597995674079595\n",
      "ep19: game finished, reward: -1.0\n",
      "17.82324\n",
      "resetting env. episode reward total was -19.0. running mean: -19.5920157173388\n",
      "ep20: game finished, reward: -1.0\n",
      "22.80538999999999\n",
      "resetting env. episode reward total was -18.0. running mean: -19.576095560165413\n",
      "ep21: game finished, reward: -1.0\n",
      "26.748000999999988\n",
      "resetting env. episode reward total was -17.0. running mean: -19.550334604563762\n",
      "ep22: game finished, reward: -1.0\n",
      "23.248338999999987\n",
      "resetting env. episode reward total was -18.0. running mean: -19.534831258518125\n",
      "ep23: game finished, reward: -1.0\n",
      "18.136402000000032\n",
      "resetting env. episode reward total was -19.0. running mean: -19.529482945932944\n",
      "ep24: game finished, reward: -1.0\n",
      "17.482736000000045\n",
      "resetting env. episode reward total was -19.0. running mean: -19.524188116473617\n",
      "ep25: game finished, reward: -1.0\n",
      "18.603512000000023\n",
      "resetting env. episode reward total was -17.0. running mean: -19.498946235308882\n",
      "ep26: game finished, reward: -1.0\n",
      "19.379246000000023\n",
      "resetting env. episode reward total was -13.0. running mean: -19.433956772955792\n",
      "ep27: game finished, reward: -1.0\n",
      "20.34531899999996\n",
      "resetting env. episode reward total was -18.0. running mean: -19.419617205226235\n",
      "ep28: game finished, reward: -1.0\n",
      "20.082772999999975\n",
      "resetting env. episode reward total was -17.0. running mean: -19.395421033173974\n",
      "ep29: game finished, reward: -1.0\n",
      "18.624310000000037\n",
      "resetting env. episode reward total was -16.0. running mean: -19.361466822842235\n",
      "ep30: game finished, reward: -1.0\n",
      "15.999265000000037\n",
      "resetting env. episode reward total was -18.0. running mean: -19.34785215461381\n",
      "ep31: game finished, reward: -1.0\n",
      "18.633677000000034\n",
      "resetting env. episode reward total was -15.0. running mean: -19.304373633067673\n",
      "ep32: game finished, reward: -1.0\n",
      "17.365648999999962\n",
      "resetting env. episode reward total was -19.0. running mean: -19.301329896737\n",
      "ep33: game finished, reward: -1.0\n",
      "19.63218699999993\n",
      "resetting env. episode reward total was -14.0. running mean: -19.24831659776963\n",
      "ep34: game finished, reward: -1.0\n",
      "15.695564999999988\n",
      "resetting env. episode reward total was -20.0. running mean: -19.255833431791935\n",
      "ep35: game finished, reward: -1.0\n",
      "22.29747599999996\n",
      "resetting env. episode reward total was -15.0. running mean: -19.213275097474014\n",
      "ep36: game finished, reward: -1.0\n",
      "19.584651000000008\n",
      "resetting env. episode reward total was -17.0. running mean: -19.191142346499277\n",
      "ep37: game finished, reward: -1.0\n",
      "18.34912399999996\n",
      "resetting env. episode reward total was -19.0. running mean: -19.189230923034284\n",
      "ep38: game finished, reward: -1.0\n",
      "17.506302000000005\n",
      "resetting env. episode reward total was -20.0. running mean: -19.19733861380394\n",
      "ep39: game finished, reward: -1.0\n",
      "16.651651000000015\n",
      "resetting env. episode reward total was -15.0. running mean: -19.1553652276659\n",
      "ep40: game finished, reward: -1.0\n",
      "19.84962500000006\n",
      "resetting env. episode reward total was -19.0. running mean: -19.15381157538924\n",
      "ep41: game finished, reward: -1.0\n",
      "21.728173999999967\n",
      "resetting env. episode reward total was -18.0. running mean: -19.142273459635348\n",
      "ep42: game finished, reward: -1.0\n",
      "16.657430999999974\n",
      "resetting env. episode reward total was -18.0. running mean: -19.130850725038993\n",
      "ep43: game finished, reward: -1.0\n",
      "13.83760200000006\n",
      "resetting env. episode reward total was -21.0. running mean: -19.149542217788603\n",
      "ep44: game finished, reward: -1.0\n",
      "15.974484999999959\n",
      "resetting env. episode reward total was -19.0. running mean: -19.148046795610718\n",
      "ep45: game finished, reward: -1.0\n",
      "18.16423599999996\n",
      "resetting env. episode reward total was -17.0. running mean: -19.12656632765461\n",
      "ep46: game finished, reward: -1.0\n",
      "19.595646999999985\n",
      "resetting env. episode reward total was -15.0. running mean: -19.085300664378064\n",
      "ep47: game finished, reward: -1.0\n",
      "16.82578000000001\n",
      "resetting env. episode reward total was -19.0. running mean: -19.084447657734284\n",
      "ep48: game finished, reward: -1.0\n",
      "14.349863000000028\n",
      "resetting env. episode reward total was -21.0. running mean: -19.10360318115694\n",
      "ep49: game finished, reward: -1.0\n",
      "16.783333999999968\n",
      "resetting env. episode reward total was -17.0. running mean: -19.082567149345373\n",
      "ep50: game finished, reward: -1.0\n",
      "20.189043999999967\n",
      "resetting env. episode reward total was -17.0. running mean: -19.06174147785192\n",
      "ep51: game finished, reward: -1.0\n",
      "19.37108699999999\n",
      "resetting env. episode reward total was -19.0. running mean: -19.061124063073404\n",
      "ep52: game finished, reward: -1.0\n",
      "17.67420500000003\n",
      "resetting env. episode reward total was -18.0. running mean: -19.05051282244267\n",
      "ep53: game finished, reward: -1.0\n",
      "25.281708999999978\n",
      "resetting env. episode reward total was -16.0. running mean: -19.020007694218243\n",
      "ep54: game finished, reward: -1.0\n",
      "15.56866500000001\n",
      "resetting env. episode reward total was -18.0. running mean: -19.00980761727606\n",
      "ep55: game finished, reward: -1.0\n",
      "16.366993000000093\n",
      "resetting env. episode reward total was -17.0. running mean: -18.989709541103302\n",
      "ep56: game finished, reward: -1.0\n",
      "16.485345999999936\n",
      "resetting env. episode reward total was -20.0. running mean: -18.999812445692267\n",
      "ep57: game finished, reward: -1.0\n",
      "18.589341000000104\n",
      "resetting env. episode reward total was -18.0. running mean: -18.989814321235343\n",
      "ep58: game finished, reward: -1.0\n",
      "16.98390399999994\n",
      "resetting env. episode reward total was -17.0. running mean: -18.96991617802299\n",
      "ep59: game finished, reward: -1.0\n",
      "14.617619999999988\n",
      "resetting env. episode reward total was -18.0. running mean: -18.960217016242762\n",
      "ep60: game finished, reward: -1.0\n",
      "21.78234199999997\n",
      "resetting env. episode reward total was -17.0. running mean: -18.940614846080337\n",
      "ep61: game finished, reward: -1.0\n",
      "16.214374000000134\n",
      "resetting env. episode reward total was -19.0. running mean: -18.941208697619533\n",
      "ep62: game finished, reward: -1.0\n",
      "17.6888899999999\n",
      "resetting env. episode reward total was -19.0. running mean: -18.94179661064334\n",
      "ep63: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.411828000000014\n",
      "resetting env. episode reward total was -18.0. running mean: -18.932378644536907\n",
      "ep64: game finished, reward: -1.0\n",
      "16.593208999999888\n",
      "resetting env. episode reward total was -19.0. running mean: -18.933054858091538\n",
      "ep65: game finished, reward: -1.0\n",
      "15.555650999999898\n",
      "resetting env. episode reward total was -18.0. running mean: -18.92372430951062\n",
      "ep66: game finished, reward: -1.0\n",
      "16.83496100000002\n",
      "resetting env. episode reward total was -17.0. running mean: -18.904487066415516\n",
      "ep67: game finished, reward: -1.0\n",
      "19.82754799999998\n",
      "resetting env. episode reward total was -17.0. running mean: -18.885442195751363\n",
      "ep68: game finished, reward: -1.0\n",
      "21.317289000000073\n",
      "resetting env. episode reward total was -15.0. running mean: -18.846587773793846\n",
      "ep69: game finished, reward: -1.0\n",
      "17.923412999999982\n",
      "resetting env. episode reward total was -15.0. running mean: -18.808121896055905\n",
      "ep70: game finished, reward: -1.0\n",
      "20.76496300000008\n",
      "resetting env. episode reward total was -17.0. running mean: -18.790040677095348\n",
      "ep71: game finished, reward: -1.0\n",
      "21.69643700000006\n",
      "resetting env. episode reward total was -19.0. running mean: -18.792140270324396\n",
      "ep72: game finished, reward: -1.0\n",
      "18.038739000000078\n",
      "resetting env. episode reward total was -17.0. running mean: -18.774218867621155\n",
      "ep73: game finished, reward: -1.0\n",
      "15.05501900000013\n",
      "resetting env. episode reward total was -18.0. running mean: -18.766476678944944\n",
      "ep74: game finished, reward: -1.0\n",
      "19.165768999999955\n",
      "resetting env. episode reward total was -17.0. running mean: -18.748811912155496\n",
      "ep75: game finished, reward: -1.0\n",
      "17.710060999999996\n",
      "resetting env. episode reward total was -21.0. running mean: -18.771323793033943\n",
      "ep76: game finished, reward: -1.0\n",
      "19.36349600000017\n",
      "resetting env. episode reward total was -18.0. running mean: -18.763610555103604\n",
      "ep77: game finished, reward: -1.0\n",
      "16.78721100000007\n",
      "resetting env. episode reward total was -17.0. running mean: -18.74597444955257\n",
      "ep78: game finished, reward: -1.0\n",
      "19.079494000000068\n",
      "resetting env. episode reward total was -20.0. running mean: -18.758514705057042\n",
      "ep79: game finished, reward: -1.0\n",
      "18.074114999999892\n",
      "resetting env. episode reward total was -18.0. running mean: -18.750929558006472\n",
      "ep80: game finished, reward: -1.0\n",
      "21.243246999999883\n",
      "resetting env. episode reward total was -15.0. running mean: -18.713420262426407\n",
      "ep81: game finished, reward: -1.0\n",
      "19.35899100000006\n",
      "resetting env. episode reward total was -15.0. running mean: -18.67628605980214\n",
      "ep82: game finished, reward: -1.0\n",
      "24.13707700000009\n",
      "resetting env. episode reward total was -17.0. running mean: -18.65952319920412\n",
      "ep83: game finished, reward: -1.0\n",
      "19.59051599999998\n",
      "resetting env. episode reward total was -20.0. running mean: -18.67292796721208\n",
      "ep84: game finished, reward: -1.0\n",
      "23.453615999999784\n",
      "resetting env. episode reward total was -17.0. running mean: -18.65619868753996\n",
      "ep85: game finished, reward: -1.0\n",
      "20.17838999999981\n",
      "resetting env. episode reward total was -19.0. running mean: -18.65963670066456\n",
      "ep86: game finished, reward: -1.0\n",
      "21.103290000000015\n",
      "resetting env. episode reward total was -19.0. running mean: -18.663040333657914\n",
      "ep87: game finished, reward: -1.0\n",
      "21.318117999999913\n",
      "resetting env. episode reward total was -19.0. running mean: -18.666409930321336\n",
      "ep88: game finished, reward: -1.0\n",
      "18.045581999999968\n",
      "resetting env. episode reward total was -17.0. running mean: -18.649745831018123\n",
      "ep89: game finished, reward: -1.0\n",
      "16.464093999999932\n",
      "resetting env. episode reward total was -19.0. running mean: -18.653248372707942\n",
      "ep90: game finished, reward: -1.0\n",
      "18.331380999999965\n",
      "resetting env. episode reward total was -19.0. running mean: -18.656715888980862\n",
      "ep91: game finished, reward: -1.0\n",
      "14.544730000000072\n",
      "resetting env. episode reward total was -21.0. running mean: -18.680148730091055\n",
      "ep92: game finished, reward: -1.0\n",
      "16.89302100000009\n",
      "resetting env. episode reward total was -19.0. running mean: -18.683347242790145\n",
      "ep93: game finished, reward: -1.0\n",
      "19.665420999999924\n",
      "resetting env. episode reward total was -19.0. running mean: -18.686513770362243\n",
      "ep94: game finished, reward: -1.0\n",
      "15.955511999999999\n",
      "resetting env. episode reward total was -21.0. running mean: -18.70964863265862\n",
      "ep95: game finished, reward: -1.0\n",
      "22.505473000000165\n",
      "resetting env. episode reward total was -17.0. running mean: -18.692552146332037\n",
      "ep96: game finished, reward: -1.0\n",
      "19.275382000000036\n",
      "resetting env. episode reward total was -18.0. running mean: -18.685626624868718\n",
      "ep97: game finished, reward: -1.0\n",
      "18.06677599999989\n",
      "resetting env. episode reward total was -17.0. running mean: -18.668770358620034\n",
      "ep98: game finished, reward: -1.0\n",
      "17.334745999999996\n",
      "resetting env. episode reward total was -15.0. running mean: -18.63208265503383\n",
      "ep99: game finished, reward: -1.0\n",
      "18.73205699999994\n",
      "resetting env. episode reward total was -17.0. running mean: -18.615761828483492\n",
      "ep100: game finished, reward: -1.0\n",
      "19.605773999999883\n",
      "resetting env. episode reward total was -15.0. running mean: -18.579604210198656\n",
      "ep101: game finished, reward: -1.0\n",
      "20.3220060000001\n",
      "resetting env. episode reward total was -19.0. running mean: -18.58380816809667\n",
      "ep102: game finished, reward: -1.0\n",
      "19.17126700000017\n",
      "resetting env. episode reward total was -17.0. running mean: -18.567970086415706\n",
      "ep103: game finished, reward: -1.0\n",
      "23.37529100000006\n",
      "resetting env. episode reward total was -19.0. running mean: -18.57229038555155\n",
      "ep104: game finished, reward: -1.0\n",
      "23.30169099999989\n",
      "resetting env. episode reward total was -19.0. running mean: -18.576567481696035\n",
      "ep105: game finished, reward: -1.0\n",
      "17.85681199999999\n",
      "resetting env. episode reward total was -20.0. running mean: -18.590801806879075\n",
      "ep106: game finished, reward: -1.0\n",
      "19.8173519999998\n",
      "resetting env. episode reward total was -21.0. running mean: -18.614893788810285\n",
      "ep107: game finished, reward: -1.0\n",
      "22.58021800000006\n",
      "resetting env. episode reward total was -16.0. running mean: -18.588744850922183\n",
      "ep108: game finished, reward: -1.0\n",
      "16.34918300000004\n",
      "resetting env. episode reward total was -19.0. running mean: -18.59285740241296\n",
      "ep109: game finished, reward: -1.0\n",
      "15.794280999999955\n",
      "resetting env. episode reward total was -21.0. running mean: -18.616928828388833\n",
      "ep110: game finished, reward: -1.0\n",
      "17.439519999999902\n",
      "resetting env. episode reward total was -15.0. running mean: -18.580759540104943\n",
      "ep111: game finished, reward: -1.0\n",
      "18.89471800000001\n",
      "resetting env. episode reward total was -19.0. running mean: -18.584951944703896\n",
      "ep112: game finished, reward: -1.0\n",
      "17.829447999999957\n",
      "resetting env. episode reward total was -13.0. running mean: -18.529102425256855\n",
      "ep113: game finished, reward: -1.0\n",
      "19.856454000000213\n",
      "resetting env. episode reward total was -16.0. running mean: -18.50381140100429\n",
      "ep114: game finished, reward: -1.0\n",
      "15.228791000000001\n",
      "resetting env. episode reward total was -21.0. running mean: -18.528773286994245\n",
      "ep115: game finished, reward: -1.0\n",
      "16.691102999999657\n",
      "resetting env. episode reward total was -19.0. running mean: -18.533485554124304\n",
      "ep116: game finished, reward: -1.0\n",
      "17.132116000000224\n",
      "resetting env. episode reward total was -19.0. running mean: -18.538150698583063\n",
      "ep117: game finished, reward: -1.0\n",
      "17.150804999999764\n",
      "resetting env. episode reward total was -19.0. running mean: -18.542769191597234\n",
      "ep118: game finished, reward: -1.0\n",
      "23.56232799999998\n",
      "resetting env. episode reward total was -16.0. running mean: -18.51734149968126\n",
      "ep119: game finished, reward: -1.0\n",
      "20.064456999999948\n",
      "resetting env. episode reward total was -15.0. running mean: -18.482168084684446\n",
      "ep120: game finished, reward: -1.0\n",
      "16.635165999999572\n",
      "resetting env. episode reward total was -18.0. running mean: -18.4773464038376\n",
      "ep121: game finished, reward: -1.0\n",
      "17.85418400000026\n",
      "resetting env. episode reward total was -21.0. running mean: -18.502572939799226\n",
      "ep122: game finished, reward: -1.0\n",
      "23.321817999999894\n",
      "resetting env. episode reward total was -15.0. running mean: -18.467547210401232\n",
      "ep123: game finished, reward: -1.0\n",
      "16.71635100000003\n",
      "resetting env. episode reward total was -19.0. running mean: -18.472871738297222\n",
      "ep124: game finished, reward: -1.0\n",
      "19.94401599999992\n",
      "resetting env. episode reward total was -16.0. running mean: -18.448143020914248\n",
      "ep125: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.483009999999922\n",
      "resetting env. episode reward total was -16.0. running mean: -18.423661590705105\n",
      "ep126: game finished, reward: -1.0\n",
      "19.262220999999954\n",
      "resetting env. episode reward total was -20.0. running mean: -18.439424974798055\n",
      "ep127: game finished, reward: -1.0\n",
      "17.7147130000003\n",
      "resetting env. episode reward total was -14.0. running mean: -18.395030725050074\n",
      "ep128: game finished, reward: -1.0\n",
      "22.341511000000082\n",
      "resetting env. episode reward total was -15.0. running mean: -18.36108041779957\n",
      "ep129: game finished, reward: -1.0\n",
      "23.476914000000306\n",
      "resetting env. episode reward total was -13.0. running mean: -18.307469613621574\n",
      "ep130: game finished, reward: -1.0\n",
      "15.706288999999742\n",
      "resetting env. episode reward total was -21.0. running mean: -18.33439491748536\n",
      "ep131: game finished, reward: -1.0\n",
      "18.157388000000083\n",
      "resetting env. episode reward total was -19.0. running mean: -18.341050968310505\n",
      "ep132: game finished, reward: -1.0\n",
      "19.295012000000042\n",
      "resetting env. episode reward total was -17.0. running mean: -18.3276404586274\n",
      "ep133: game finished, reward: -1.0\n",
      "23.77946699999984\n",
      "resetting env. episode reward total was -17.0. running mean: -18.31436405404113\n",
      "ep134: game finished, reward: -1.0\n",
      "17.568464999999833\n",
      "resetting env. episode reward total was -21.0. running mean: -18.341220413500718\n",
      "ep135: game finished, reward: -1.0\n",
      "22.662345000000187\n",
      "resetting env. episode reward total was -15.0. running mean: -18.30780820936571\n",
      "ep136: game finished, reward: -1.0\n",
      "14.11916199999996\n",
      "resetting env. episode reward total was -20.0. running mean: -18.32473012727205\n",
      "ep137: game finished, reward: -1.0\n",
      "17.151593999999932\n",
      "resetting env. episode reward total was -19.0. running mean: -18.33148282599933\n",
      "ep138: game finished, reward: -1.0\n",
      "19.026953999999932\n",
      "resetting env. episode reward total was -17.0. running mean: -18.318167997739337\n",
      "ep139: game finished, reward: -1.0\n",
      "19.14189699999997\n",
      "resetting env. episode reward total was -19.0. running mean: -18.324986317761944\n",
      "ep140: game finished, reward: -1.0\n",
      "15.564008000000285\n",
      "resetting env. episode reward total was -18.0. running mean: -18.321736454584325\n",
      "ep141: game finished, reward: -1.0\n",
      "23.965631999999914\n",
      "resetting env. episode reward total was -11.0. running mean: -18.24851909003848\n",
      "ep142: game finished, reward: -1.0\n",
      "18.70507600000019\n",
      "resetting env. episode reward total was -17.0. running mean: -18.2360338991381\n",
      "ep143: game finished, reward: -1.0\n",
      "20.020070999999916\n",
      "resetting env. episode reward total was -19.0. running mean: -18.24367356014672\n",
      "ep144: game finished, reward: -1.0\n",
      "16.98436700000002\n",
      "resetting env. episode reward total was -16.0. running mean: -18.221236824545255\n",
      "ep145: game finished, reward: -1.0\n",
      "19.808268000000226\n",
      "resetting env. episode reward total was -18.0. running mean: -18.219024456299802\n",
      "ep146: game finished, reward: -1.0\n",
      "19.730577000000267\n",
      "resetting env. episode reward total was -15.0. running mean: -18.1868342117368\n",
      "ep147: game finished, reward: -1.0\n",
      "19.39936099999977\n",
      "resetting env. episode reward total was -17.0. running mean: -18.174965869619434\n",
      "ep148: game finished, reward: -1.0\n",
      "17.431338999999753\n",
      "resetting env. episode reward total was -18.0. running mean: -18.17321621092324\n",
      "ep149: game finished, reward: -1.0\n",
      "17.719164999999975\n",
      "resetting env. episode reward total was -19.0. running mean: -18.18148404881401\n",
      "ep150: game finished, reward: -1.0\n",
      "16.10674599999993\n",
      "resetting env. episode reward total was -21.0. running mean: -18.20966920832587\n",
      "ep151: game finished, reward: -1.0\n",
      "24.332742000000053\n",
      "resetting env. episode reward total was -15.0. running mean: -18.17757251624261\n",
      "ep152: game finished, reward: -1.0\n",
      "21.636320999999953\n",
      "resetting env. episode reward total was -18.0. running mean: -18.17579679108018\n",
      "ep153: game finished, reward: -1.0\n",
      "17.57273699999996\n",
      "resetting env. episode reward total was -13.0. running mean: -18.12403882316938\n",
      "ep154: game finished, reward: -1.0\n",
      "18.767503999999917\n",
      "resetting env. episode reward total was -17.0. running mean: -18.112798434937687\n",
      "ep155: game finished, reward: -1.0\n",
      "20.067009999999755\n",
      "resetting env. episode reward total was -20.0. running mean: -18.13167045058831\n",
      "ep156: game finished, reward: -1.0\n",
      "19.662909000000127\n",
      "resetting env. episode reward total was -21.0. running mean: -18.16035374608243\n",
      "ep157: game finished, reward: -1.0\n",
      "15.854137000000264\n",
      "resetting env. episode reward total was -19.0. running mean: -18.168750208621606\n",
      "ep158: game finished, reward: -1.0\n",
      "21.663614000000052\n",
      "resetting env. episode reward total was -14.0. running mean: -18.12706270653539\n",
      "ep159: game finished, reward: -1.0\n",
      "17.786203000000114\n",
      "resetting env. episode reward total was -17.0. running mean: -18.115792079470037\n",
      "ep160: game finished, reward: -1.0\n",
      "15.757841999999982\n",
      "resetting env. episode reward total was -19.0. running mean: -18.124634158675338\n",
      "ep161: game finished, reward: -1.0\n",
      "21.723811000000296\n",
      "resetting env. episode reward total was -17.0. running mean: -18.113387817088586\n",
      "ep162: game finished, reward: -1.0\n",
      "19.735842000000048\n",
      "resetting env. episode reward total was -16.0. running mean: -18.0922539389177\n",
      "ep163: game finished, reward: -1.0\n",
      "17.107066999999915\n",
      "resetting env. episode reward total was -19.0. running mean: -18.101331399528526\n",
      "ep164: game finished, reward: -1.0\n",
      "21.65231900000026\n",
      "resetting env. episode reward total was -17.0. running mean: -18.09031808553324\n",
      "ep165: game finished, reward: -1.0\n",
      "21.260963000000174\n",
      "resetting env. episode reward total was -17.0. running mean: -18.07941490467791\n",
      "ep166: game finished, reward: -1.0\n",
      "26.234849999999824\n",
      "resetting env. episode reward total was -15.0. running mean: -18.04862075563113\n",
      "ep167: game finished, reward: -1.0\n",
      "18.780647000000044\n",
      "resetting env. episode reward total was -21.0. running mean: -18.07813454807482\n",
      "ep168: game finished, reward: -1.0\n",
      "20.04423799999995\n",
      "resetting env. episode reward total was -17.0. running mean: -18.067353202594074\n",
      "ep169: game finished, reward: -1.0\n",
      "25.181536999999935\n",
      "resetting env. episode reward total was -13.0. running mean: -18.01667967056813\n",
      "ep170: game finished, reward: -1.0\n",
      "16.756808999999976\n",
      "resetting env. episode reward total was -21.0. running mean: -18.04651287386245\n",
      "ep171: game finished, reward: -1.0\n",
      "17.035617999999886\n",
      "resetting env. episode reward total was -21.0. running mean: -18.076047745123827\n",
      "ep172: game finished, reward: -1.0\n",
      "19.14342600000009\n",
      "resetting env. episode reward total was -19.0. running mean: -18.08528726767259\n",
      "ep173: game finished, reward: -1.0\n",
      "20.53623800000014\n",
      "resetting env. episode reward total was -17.0. running mean: -18.074434394995865\n",
      "ep174: game finished, reward: -1.0\n",
      "23.663923000000068\n",
      "resetting env. episode reward total was -15.0. running mean: -18.043690051045907\n",
      "ep175: game finished, reward: -1.0\n",
      "16.921162000000095\n",
      "resetting env. episode reward total was -17.0. running mean: -18.03325315053545\n",
      "ep176: game finished, reward: -1.0\n",
      "16.153953\n",
      "resetting env. episode reward total was -19.0. running mean: -18.0429206190301\n",
      "ep177: game finished, reward: -1.0\n",
      "18.826602999999977\n",
      "resetting env. episode reward total was -15.0. running mean: -18.012491412839797\n",
      "ep178: game finished, reward: -1.0\n",
      "15.531280000000152\n",
      "resetting env. episode reward total was -18.0. running mean: -18.012366498711398\n",
      "ep179: game finished, reward: -1.0\n",
      "22.011993000000075\n",
      "resetting env. episode reward total was -14.0. running mean: -17.972242833724284\n",
      "ep180: game finished, reward: -1.0\n",
      "24.662402999999813\n",
      "resetting env. episode reward total was -17.0. running mean: -17.962520405387043\n",
      "ep181: game finished, reward: -1.0\n",
      "17.703171999999995\n",
      "resetting env. episode reward total was -20.0. running mean: -17.98289520133317\n",
      "ep182: game finished, reward: -1.0\n",
      "22.36125099999981\n",
      "resetting env. episode reward total was -16.0. running mean: -17.96306624931984\n",
      "ep183: game finished, reward: -1.0\n",
      "21.047476000000188\n",
      "resetting env. episode reward total was -16.0. running mean: -17.94343558682664\n",
      "ep184: game finished, reward: -1.0\n",
      "18.68634800000018\n",
      "resetting env. episode reward total was -18.0. running mean: -17.944001230958374\n",
      "ep185: game finished, reward: -1.0\n",
      "18.557749999999942\n",
      "resetting env. episode reward total was -21.0. running mean: -17.97456121864879\n",
      "ep186: game finished, reward: -1.0\n",
      "18.17015500000025\n",
      "resetting env. episode reward total was -16.0. running mean: -17.954815606462304\n",
      "ep187: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.80800999999974\n",
      "resetting env. episode reward total was -11.0. running mean: -17.88526745039768\n",
      "ep188: game finished, reward: -1.0\n",
      "18.348696000000018\n",
      "resetting env. episode reward total was -19.0. running mean: -17.896414775893707\n",
      "ep189: game finished, reward: -1.0\n",
      "15.872961999999916\n",
      "resetting env. episode reward total was -17.0. running mean: -17.887450628134772\n",
      "ep190: game finished, reward: -1.0\n",
      "22.625714000000244\n",
      "resetting env. episode reward total was -15.0. running mean: -17.858576121853424\n",
      "ep191: game finished, reward: -1.0\n",
      "18.503211000000192\n",
      "resetting env. episode reward total was -17.0. running mean: -17.84999036063489\n",
      "ep192: game finished, reward: -1.0\n",
      "17.112307999999757\n",
      "resetting env. episode reward total was -17.0. running mean: -17.841490457028545\n",
      "ep193: game finished, reward: -1.0\n",
      "14.618379000000004\n",
      "resetting env. episode reward total was -19.0. running mean: -17.85307555245826\n",
      "ep194: game finished, reward: -1.0\n",
      "15.737039999999979\n",
      "resetting env. episode reward total was -19.0. running mean: -17.86454479693368\n",
      "ep195: game finished, reward: -1.0\n",
      "18.62318200000027\n",
      "resetting env. episode reward total was -13.0. running mean: -17.81589934896434\n",
      "ep196: game finished, reward: -1.0\n",
      "18.967212000000018\n",
      "resetting env. episode reward total was -17.0. running mean: -17.8077403554747\n",
      "ep197: game finished, reward: -1.0\n",
      "18.545202000000245\n",
      "resetting env. episode reward total was -19.0. running mean: -17.819662951919952\n",
      "ep198: game finished, reward: -1.0\n",
      "19.28941599999962\n",
      "resetting env. episode reward total was -19.0. running mean: -17.831466322400754\n",
      "ep199: game finished, reward: -1.0\n",
      "20.282975999999962\n",
      "resetting env. episode reward total was -15.0. running mean: -17.803151659176745\n",
      "ep200: game finished, reward: -1.0\n",
      "18.585925000000316\n",
      "resetting env. episode reward total was -19.0. running mean: -17.815120142584977\n",
      "ep201: game finished, reward: -1.0\n",
      "17.56086699999969\n",
      "resetting env. episode reward total was -19.0. running mean: -17.82696894115913\n",
      "ep202: game finished, reward: -1.0\n",
      "15.835849999999937\n",
      "resetting env. episode reward total was -21.0. running mean: -17.858699251747538\n",
      "ep203: game finished, reward: -1.0\n",
      "14.791009000000031\n",
      "resetting env. episode reward total was -19.0. running mean: -17.870112259230062\n",
      "ep204: game finished, reward: -1.0\n",
      "19.653015999999752\n",
      "resetting env. episode reward total was -19.0. running mean: -17.881411136637762\n",
      "ep205: game finished, reward: -1.0\n",
      "18.691878000000088\n",
      "resetting env. episode reward total was -19.0. running mean: -17.892597025271385\n",
      "ep206: game finished, reward: -1.0\n",
      "15.76511100000016\n",
      "resetting env. episode reward total was -17.0. running mean: -17.883671055018674\n",
      "ep207: game finished, reward: -1.0\n",
      "17.21483699999999\n",
      "resetting env. episode reward total was -21.0. running mean: -17.91483434446849\n",
      "ep208: game finished, reward: -1.0\n",
      "18.667578999999932\n",
      "resetting env. episode reward total was -19.0. running mean: -17.925686001023806\n",
      "ep209: game finished, reward: -1.0\n",
      "16.875700999999935\n",
      "resetting env. episode reward total was -16.0. running mean: -17.906429141013568\n",
      "ep210: game finished, reward: -1.0\n",
      "22.504401999999573\n",
      "resetting env. episode reward total was -19.0. running mean: -17.917364849603434\n",
      "ep211: game finished, reward: -1.0\n",
      "26.518268000000262\n",
      "resetting env. episode reward total was -17.0. running mean: -17.9081912011074\n",
      "ep212: game finished, reward: -1.0\n",
      "20.4197079999999\n",
      "resetting env. episode reward total was -21.0. running mean: -17.939109289096326\n",
      "ep213: game finished, reward: -1.0\n",
      "24.514995999999883\n",
      "resetting env. episode reward total was -17.0. running mean: -17.929718196205364\n",
      "ep214: game finished, reward: -1.0\n",
      "24.020986999999877\n",
      "resetting env. episode reward total was -17.0. running mean: -17.920421014243313\n",
      "ep215: game finished, reward: -1.0\n",
      "18.49140699999998\n",
      "resetting env. episode reward total was -21.0. running mean: -17.95121680410088\n",
      "ep216: game finished, reward: -1.0\n",
      "20.18498099999988\n",
      "resetting env. episode reward total was -18.0. running mean: -17.951704636059873\n",
      "ep217: game finished, reward: -1.0\n",
      "19.499824000000444\n",
      "resetting env. episode reward total was -19.0. running mean: -17.962187589699276\n",
      "ep218: game finished, reward: -1.0\n",
      "20.741065000000162\n",
      "resetting env. episode reward total was -17.0. running mean: -17.952565713802286\n",
      "ep219: game finished, reward: -1.0\n",
      "19.773033999999825\n",
      "resetting env. episode reward total was -17.0. running mean: -17.943040056664266\n",
      "ep220: game finished, reward: -1.0\n",
      "21.909677999999985\n",
      "resetting env. episode reward total was -18.0. running mean: -17.943609656097625\n",
      "ep221: game finished, reward: -1.0\n",
      "20.19308799999999\n",
      "resetting env. episode reward total was -18.0. running mean: -17.944173559536647\n",
      "ep222: game finished, reward: -1.0\n",
      "19.58432999999968\n",
      "resetting env. episode reward total was -18.0. running mean: -17.94473182394128\n",
      "ep223: game finished, reward: -1.0\n",
      "21.305755999999747\n",
      "resetting env. episode reward total was -17.0. running mean: -17.93528450570187\n",
      "ep224: game finished, reward: -1.0\n",
      "20.338479000000007\n",
      "resetting env. episode reward total was -15.0. running mean: -17.90593166064485\n",
      "ep225: game finished, reward: -1.0\n",
      "18.37270399999943\n",
      "resetting env. episode reward total was -17.0. running mean: -17.896872344038403\n",
      "ep226: game finished, reward: -1.0\n",
      "19.198731999999836\n",
      "resetting env. episode reward total was -19.0. running mean: -17.90790362059802\n",
      "ep227: game finished, reward: -1.0\n",
      "19.809508999999707\n",
      "resetting env. episode reward total was -17.0. running mean: -17.89882458439204\n",
      "ep228: game finished, reward: -1.0\n",
      "17.8429900000001\n",
      "resetting env. episode reward total was -15.0. running mean: -17.86983633854812\n",
      "ep229: game finished, reward: -1.0\n",
      "23.023165999999947\n",
      "resetting env. episode reward total was -17.0. running mean: -17.861137975162638\n",
      "ep230: game finished, reward: -1.0\n",
      "20.008066999999755\n",
      "resetting env. episode reward total was -16.0. running mean: -17.842526595411012\n",
      "ep231: game finished, reward: -1.0\n",
      "22.034552999999505\n",
      "resetting env. episode reward total was -16.0. running mean: -17.8241013294569\n",
      "ep232: game finished, reward: -1.0\n",
      "25.468769000000066\n",
      "resetting env. episode reward total was -17.0. running mean: -17.815860316162333\n",
      "ep233: game finished, reward: -1.0\n",
      "24.146230999999716\n",
      "resetting env. episode reward total was -19.0. running mean: -17.82770171300071\n",
      "ep234: game finished, reward: -1.0\n",
      "16.10225200000059\n",
      "resetting env. episode reward total was -21.0. running mean: -17.859424695870704\n",
      "ep235: game finished, reward: -1.0\n",
      "21.740372000000207\n",
      "resetting env. episode reward total was -17.0. running mean: -17.850830448912\n",
      "ep236: game finished, reward: -1.0\n",
      "26.133511\n",
      "resetting env. episode reward total was -16.0. running mean: -17.83232214442288\n",
      "ep237: game finished, reward: -1.0\n",
      "27.610752000000502\n",
      "resetting env. episode reward total was -13.0. running mean: -17.78399892297865\n",
      "ep238: game finished, reward: -1.0\n",
      "21.971016000000418\n",
      "resetting env. episode reward total was -19.0. running mean: -17.796158933748863\n",
      "ep239: game finished, reward: -1.0\n",
      "22.110662000000048\n",
      "resetting env. episode reward total was -15.0. running mean: -17.768197344411373\n",
      "ep240: game finished, reward: -1.0\n",
      "17.131570999999894\n",
      "resetting env. episode reward total was -20.0. running mean: -17.79051537096726\n",
      "ep241: game finished, reward: -1.0\n",
      "17.228957000000264\n",
      "resetting env. episode reward total was -19.0. running mean: -17.802610217257588\n",
      "ep242: game finished, reward: -1.0\n",
      "24.228344999999536\n",
      "resetting env. episode reward total was -10.0. running mean: -17.724584115085012\n",
      "ep243: game finished, reward: -1.0\n",
      "17.849526999999398\n",
      "resetting env. episode reward total was -19.0. running mean: -17.737338273934164\n",
      "ep244: game finished, reward: -1.0\n",
      "25.113094999999703\n",
      "resetting env. episode reward total was -17.0. running mean: -17.72996489119482\n",
      "ep245: game finished, reward: -1.0\n",
      "22.656628000000637\n",
      "resetting env. episode reward total was -18.0. running mean: -17.732665242282874\n",
      "ep246: game finished, reward: -1.0\n",
      "19.205235000000357\n",
      "resetting env. episode reward total was -18.0. running mean: -17.735338589860046\n",
      "ep247: game finished, reward: -1.0\n",
      "19.40258399999948\n",
      "resetting env. episode reward total was -18.0. running mean: -17.737985203961443\n",
      "ep248: game finished, reward: -1.0\n",
      "22.314636999999493\n",
      "resetting env. episode reward total was -13.0. running mean: -17.69060535192183\n",
      "ep249: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.525101999999606\n",
      "resetting env. episode reward total was -15.0. running mean: -17.66369929840261\n",
      "ep250: game finished, reward: -1.0\n",
      "18.554716999999982\n",
      "resetting env. episode reward total was -20.0. running mean: -17.687062305418582\n",
      "ep251: game finished, reward: -1.0\n",
      "16.897711000000527\n",
      "resetting env. episode reward total was -18.0. running mean: -17.690191682364397\n",
      "ep252: game finished, reward: -1.0\n",
      "17.154473999999936\n",
      "resetting env. episode reward total was -21.0. running mean: -17.723289765540752\n",
      "ep253: game finished, reward: -1.0\n",
      "19.015087000000676\n",
      "resetting env. episode reward total was -16.0. running mean: -17.706056867885344\n",
      "ep254: game finished, reward: -1.0\n",
      "19.40125799999987\n",
      "resetting env. episode reward total was -15.0. running mean: -17.67899629920649\n",
      "ep255: game finished, reward: -1.0\n",
      "20.12736500000028\n",
      "resetting env. episode reward total was -17.0. running mean: -17.672206336214426\n",
      "ep256: game finished, reward: -1.0\n",
      "20.92939800000022\n",
      "resetting env. episode reward total was -18.0. running mean: -17.67548427285228\n",
      "ep257: game finished, reward: -1.0\n",
      "20.118162000000666\n",
      "resetting env. episode reward total was -20.0. running mean: -17.698729430123755\n",
      "ep258: game finished, reward: -1.0\n",
      "19.472983999999997\n",
      "resetting env. episode reward total was -16.0. running mean: -17.681742135822518\n",
      "ep259: game finished, reward: -1.0\n",
      "19.76183200000014\n",
      "resetting env. episode reward total was -12.0. running mean: -17.624924714464292\n",
      "ep260: game finished, reward: -1.0\n",
      "18.16356799999994\n",
      "resetting env. episode reward total was -20.0. running mean: -17.64867546731965\n",
      "ep261: game finished, reward: -1.0\n",
      "17.251134000000093\n",
      "resetting env. episode reward total was -18.0. running mean: -17.652188712646453\n",
      "ep262: game finished, reward: -1.0\n",
      "21.748617999999624\n",
      "resetting env. episode reward total was -15.0. running mean: -17.625666825519986\n",
      "ep263: game finished, reward: -1.0\n",
      "23.806626999999935\n",
      "resetting env. episode reward total was -17.0. running mean: -17.619410157264788\n",
      "ep264: game finished, reward: -1.0\n",
      "23.28647799999999\n",
      "resetting env. episode reward total was -13.0. running mean: -17.57321605569214\n",
      "ep265: game finished, reward: -1.0\n",
      "20.242284999999356\n",
      "resetting env. episode reward total was -16.0. running mean: -17.557483895135217\n",
      "ep266: game finished, reward: -1.0\n",
      "19.735585999999785\n",
      "resetting env. episode reward total was -20.0. running mean: -17.581909056183864\n",
      "ep267: game finished, reward: -1.0\n",
      "22.66035499999998\n",
      "resetting env. episode reward total was -15.0. running mean: -17.556089965622025\n",
      "ep268: game finished, reward: -1.0\n",
      "27.74154400000043\n",
      "resetting env. episode reward total was -8.0. running mean: -17.460529065965805\n",
      "ep269: game finished, reward: -1.0\n",
      "19.574869000000035\n",
      "resetting env. episode reward total was -17.0. running mean: -17.455923775306147\n",
      "ep270: game finished, reward: -1.0\n",
      "20.495299999999588\n",
      "resetting env. episode reward total was -19.0. running mean: -17.471364537553086\n",
      "ep271: game finished, reward: -1.0\n",
      "16.042013000000225\n",
      "resetting env. episode reward total was -17.0. running mean: -17.466650892177558\n",
      "ep272: game finished, reward: -1.0\n",
      "21.62072800000078\n",
      "resetting env. episode reward total was -17.0. running mean: -17.461984383255786\n",
      "ep273: game finished, reward: -1.0\n",
      "17.6695150000005\n",
      "resetting env. episode reward total was -19.0. running mean: -17.477364539423228\n",
      "ep274: game finished, reward: -1.0\n",
      "19.157900999999583\n",
      "resetting env. episode reward total was -17.0. running mean: -17.472590894028997\n",
      "ep275: game finished, reward: -1.0\n",
      "16.880052000000433\n",
      "resetting env. episode reward total was -19.0. running mean: -17.48786498508871\n",
      "ep276: game finished, reward: -1.0\n",
      "16.01361100000031\n",
      "resetting env. episode reward total was -18.0. running mean: -17.492986335237823\n",
      "ep277: game finished, reward: -1.0\n",
      "18.219500000000153\n",
      "resetting env. episode reward total was -17.0. running mean: -17.488056471885447\n",
      "ep278: game finished, reward: -1.0\n",
      "22.711911999999757\n",
      "resetting env. episode reward total was -19.0. running mean: -17.503175907166593\n",
      "ep279: game finished, reward: -1.0\n",
      "19.358044999999947\n",
      "resetting env. episode reward total was -17.0. running mean: -17.49814414809493\n",
      "ep280: game finished, reward: -1.0\n",
      "19.510543000000325\n",
      "resetting env. episode reward total was -18.0. running mean: -17.50316270661398\n",
      "ep281: game finished, reward: -1.0\n",
      "19.27817199999936\n",
      "resetting env. episode reward total was -19.0. running mean: -17.518131079547842\n",
      "ep282: game finished, reward: -1.0\n",
      "21.39354699999967\n",
      "resetting env. episode reward total was -19.0. running mean: -17.532949768752363\n",
      "ep283: game finished, reward: -1.0\n",
      "20.228289000000586\n",
      "resetting env. episode reward total was -18.0. running mean: -17.53762027106484\n",
      "ep284: game finished, reward: -1.0\n",
      "18.564538999999968\n",
      "resetting env. episode reward total was -16.0. running mean: -17.52224406835419\n",
      "ep285: game finished, reward: -1.0\n",
      "19.373257999999623\n",
      "resetting env. episode reward total was -19.0. running mean: -17.53702162767065\n",
      "ep286: game finished, reward: -1.0\n",
      "18.75341799999933\n",
      "resetting env. episode reward total was -19.0. running mean: -17.551651411393944\n",
      "ep287: game finished, reward: -1.0\n",
      "22.070278999999573\n",
      "resetting env. episode reward total was -19.0. running mean: -17.566134897280005\n",
      "ep288: game finished, reward: -1.0\n",
      "23.5906559999994\n",
      "resetting env. episode reward total was -13.0. running mean: -17.520473548307205\n",
      "ep289: game finished, reward: -1.0\n",
      "19.650697000000036\n",
      "resetting env. episode reward total was -17.0. running mean: -17.515268812824136\n",
      "ep290: game finished, reward: -1.0\n",
      "15.80221100000017\n",
      "resetting env. episode reward total was -18.0. running mean: -17.520116124695893\n",
      "ep291: game finished, reward: -1.0\n",
      "18.50940199999968\n",
      "resetting env. episode reward total was -19.0. running mean: -17.534914963448934\n",
      "ep292: game finished, reward: -1.0\n",
      "20.460849000000053\n",
      "resetting env. episode reward total was -20.0. running mean: -17.559565813814444\n",
      "ep293: game finished, reward: -1.0\n",
      "22.87303199999951\n",
      "resetting env. episode reward total was -16.0. running mean: -17.5439701556763\n",
      "ep294: game finished, reward: -1.0\n",
      "20.75123100000019\n",
      "resetting env. episode reward total was -17.0. running mean: -17.538530454119538\n",
      "ep295: game finished, reward: -1.0\n",
      "19.49252499999966\n",
      "resetting env. episode reward total was -19.0. running mean: -17.553145149578345\n",
      "ep296: game finished, reward: -1.0\n",
      "27.95769299999938\n",
      "resetting env. episode reward total was -15.0. running mean: -17.52761369808256\n",
      "ep297: game finished, reward: -1.0\n",
      "24.820838999999978\n",
      "resetting env. episode reward total was -19.0. running mean: -17.542337561101736\n",
      "ep298: game finished, reward: -1.0\n",
      "16.040156999999454\n",
      "resetting env. episode reward total was -21.0. running mean: -17.57691418549072\n",
      "ep299: game finished, reward: -1.0\n",
      "20.932549000000108\n",
      "resetting env. episode reward total was -19.0. running mean: -17.591145043635812\n",
      "ep300: game finished, reward: -1.0\n",
      "21.38365000000067\n",
      "resetting env. episode reward total was -16.0. running mean: -17.575233593199453\n",
      "ep301: game finished, reward: -1.0\n",
      "21.492957000000388\n",
      "resetting env. episode reward total was -19.0. running mean: -17.589481257267458\n",
      "ep302: game finished, reward: -1.0\n",
      "22.352130000000216\n",
      "resetting env. episode reward total was -18.0. running mean: -17.593586444694783\n",
      "ep303: game finished, reward: -1.0\n",
      "22.768772999999783\n",
      "resetting env. episode reward total was -15.0. running mean: -17.567650580247832\n",
      "ep304: game finished, reward: -1.0\n",
      "21.813372999999956\n",
      "resetting env. episode reward total was -18.0. running mean: -17.571974074445354\n",
      "ep305: game finished, reward: -1.0\n",
      "20.38698000000022\n",
      "resetting env. episode reward total was -19.0. running mean: -17.5862543337009\n",
      "ep306: game finished, reward: -1.0\n",
      "17.566523000000416\n",
      "resetting env. episode reward total was -20.0. running mean: -17.61039179036389\n",
      "ep307: game finished, reward: -1.0\n",
      "16.31809599999997\n",
      "resetting env. episode reward total was -19.0. running mean: -17.624287872460254\n",
      "ep308: game finished, reward: -1.0\n",
      "19.38531299999977\n",
      "resetting env. episode reward total was -17.0. running mean: -17.61804499373565\n",
      "ep309: game finished, reward: -1.0\n",
      "25.011074000000008\n",
      "resetting env. episode reward total was -11.0. running mean: -17.551864543798295\n",
      "ep310: game finished, reward: -1.0\n",
      "15.943448999999418\n",
      "resetting env. episode reward total was -18.0. running mean: -17.556345898360313\n",
      "ep311: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.000911999999516\n",
      "resetting env. episode reward total was -17.0. running mean: -17.55078243937671\n",
      "ep312: game finished, reward: -1.0\n",
      "16.676277000000482\n",
      "resetting env. episode reward total was -19.0. running mean: -17.565274614982943\n",
      "ep313: game finished, reward: -1.0\n",
      "16.840368999999555\n",
      "resetting env. episode reward total was -19.0. running mean: -17.579621868833115\n",
      "ep314: game finished, reward: -1.0\n",
      "18.789828999999372\n",
      "resetting env. episode reward total was -16.0. running mean: -17.563825650144786\n",
      "ep315: game finished, reward: -1.0\n",
      "18.429478000000017\n",
      "resetting env. episode reward total was -18.0. running mean: -17.56818739364334\n",
      "ep316: game finished, reward: -1.0\n",
      "17.2625850000004\n",
      "resetting env. episode reward total was -18.0. running mean: -17.572505519706905\n",
      "ep317: game finished, reward: -1.0\n",
      "21.65605400000004\n",
      "resetting env. episode reward total was -17.0. running mean: -17.566780464509836\n",
      "ep318: game finished, reward: -1.0\n",
      "16.033110999999735\n",
      "resetting env. episode reward total was -19.0. running mean: -17.58111265986474\n",
      "ep319: game finished, reward: -1.0\n",
      "23.605988000000252\n",
      "resetting env. episode reward total was -19.0. running mean: -17.595301533266092\n",
      "ep320: game finished, reward: -1.0\n",
      "22.118664999999964\n",
      "resetting env. episode reward total was -17.0. running mean: -17.589348517933434\n",
      "ep321: game finished, reward: -1.0\n",
      "19.44347800000014\n",
      "resetting env. episode reward total was -17.0. running mean: -17.583455032754102\n",
      "ep322: game finished, reward: -1.0\n",
      "17.819620000000214\n",
      "resetting env. episode reward total was -21.0. running mean: -17.617620482426563\n",
      "ep323: game finished, reward: -1.0\n",
      "19.894914999999855\n",
      "resetting env. episode reward total was -14.0. running mean: -17.581444277602298\n",
      "ep324: game finished, reward: -1.0\n",
      "19.869861999999557\n",
      "resetting env. episode reward total was -16.0. running mean: -17.565629834826275\n",
      "ep325: game finished, reward: -1.0\n",
      "17.726071000000047\n",
      "resetting env. episode reward total was -19.0. running mean: -17.579973536478015\n",
      "ep326: game finished, reward: -1.0\n",
      "20.86166200000025\n",
      "resetting env. episode reward total was -17.0. running mean: -17.574173801113236\n",
      "ep327: game finished, reward: -1.0\n",
      "17.63749199999984\n",
      "resetting env. episode reward total was -18.0. running mean: -17.578432063102102\n",
      "ep328: game finished, reward: -1.0\n",
      "18.042717999999695\n",
      "resetting env. episode reward total was -21.0. running mean: -17.612647742471083\n",
      "ep329: game finished, reward: -1.0\n",
      "22.61340999999993\n",
      "resetting env. episode reward total was -17.0. running mean: -17.606521265046375\n",
      "ep330: game finished, reward: -1.0\n",
      "20.972937000000456\n",
      "resetting env. episode reward total was -19.0. running mean: -17.620456052395912\n",
      "ep331: game finished, reward: -1.0\n",
      "17.973903000000064\n",
      "resetting env. episode reward total was -19.0. running mean: -17.634251491871954\n",
      "ep332: game finished, reward: -1.0\n",
      "22.745767000000342\n",
      "resetting env. episode reward total was -14.0. running mean: -17.597908976953235\n",
      "ep333: game finished, reward: -1.0\n",
      "20.070509999999558\n",
      "resetting env. episode reward total was -17.0. running mean: -17.591929887183703\n",
      "ep334: game finished, reward: -1.0\n",
      "18.172198000000208\n",
      "resetting env. episode reward total was -19.0. running mean: -17.606010588311868\n",
      "ep335: game finished, reward: -1.0\n",
      "19.281267999999727\n",
      "resetting env. episode reward total was -19.0. running mean: -17.61995048242875\n",
      "ep336: game finished, reward: -1.0\n",
      "15.476175999999214\n",
      "resetting env. episode reward total was -20.0. running mean: -17.643750977604462\n",
      "ep337: game finished, reward: -1.0\n",
      "18.032335000000785\n",
      "resetting env. episode reward total was -16.0. running mean: -17.62731346782842\n",
      "ep338: game finished, reward: -1.0\n",
      "16.142424000000574\n",
      "resetting env. episode reward total was -18.0. running mean: -17.631040333150136\n",
      "ep339: game finished, reward: -1.0\n",
      "17.60540899999978\n",
      "resetting env. episode reward total was -20.0. running mean: -17.654729929818632\n",
      "ep340: game finished, reward: -1.0\n",
      "16.7868610000005\n",
      "resetting env. episode reward total was -19.0. running mean: -17.668182630520448\n",
      "ep341: game finished, reward: -1.0\n",
      "15.956803999999465\n",
      "resetting env. episode reward total was -19.0. running mean: -17.681500804215244\n",
      "ep342: game finished, reward: -1.0\n",
      "19.350220000000263\n",
      "resetting env. episode reward total was -20.0. running mean: -17.70468579617309\n",
      "ep343: game finished, reward: -1.0\n",
      "20.5801790000005\n",
      "resetting env. episode reward total was -18.0. running mean: -17.70763893821136\n",
      "ep344: game finished, reward: -1.0\n",
      "18.13967800000046\n",
      "resetting env. episode reward total was -20.0. running mean: -17.730562548829244\n",
      "ep345: game finished, reward: -1.0\n",
      "14.834295999999995\n",
      "resetting env. episode reward total was -21.0. running mean: -17.763256923340954\n",
      "ep346: game finished, reward: -1.0\n",
      "17.031872000000476\n",
      "resetting env. episode reward total was -17.0. running mean: -17.755624354107546\n",
      "ep347: game finished, reward: -1.0\n",
      "20.575106000000233\n",
      "resetting env. episode reward total was -17.0. running mean: -17.74806811056647\n",
      "ep348: game finished, reward: -1.0\n",
      "21.219882999999754\n",
      "resetting env. episode reward total was -15.0. running mean: -17.720587429460807\n",
      "ep349: game finished, reward: -1.0\n",
      "17.113299000000552\n",
      "resetting env. episode reward total was -21.0. running mean: -17.7533815551662\n",
      "ep350: game finished, reward: -1.0\n",
      "15.872942999999395\n",
      "resetting env. episode reward total was -21.0. running mean: -17.78584773961454\n",
      "ep351: game finished, reward: -1.0\n",
      "18.182738999999856\n",
      "resetting env. episode reward total was -19.0. running mean: -17.797989262218394\n",
      "ep352: game finished, reward: -1.0\n",
      "20.283540999999786\n",
      "resetting env. episode reward total was -19.0. running mean: -17.810009369596212\n",
      "ep353: game finished, reward: -1.0\n",
      "19.83703400000013\n",
      "resetting env. episode reward total was -15.0. running mean: -17.781909275900247\n",
      "ep354: game finished, reward: -1.0\n",
      "20.063347999999678\n",
      "resetting env. episode reward total was -19.0. running mean: -17.794090183141247\n",
      "ep355: game finished, reward: -1.0\n",
      "19.006843999999546\n",
      "resetting env. episode reward total was -16.0. running mean: -17.776149281309834\n",
      "ep356: game finished, reward: -1.0\n",
      "19.203495999999177\n",
      "resetting env. episode reward total was -19.0. running mean: -17.78838778849674\n",
      "ep357: game finished, reward: -1.0\n",
      "19.337586999999985\n",
      "resetting env. episode reward total was -14.0. running mean: -17.750503910611773\n",
      "ep358: game finished, reward: -1.0\n",
      "22.03867900000023\n",
      "resetting env. episode reward total was -15.0. running mean: -17.722998871505652\n",
      "ep359: game finished, reward: -1.0\n",
      "24.153354000000036\n",
      "resetting env. episode reward total was -17.0. running mean: -17.7157688827906\n",
      "ep360: game finished, reward: -1.0\n",
      "23.260519999999815\n",
      "resetting env. episode reward total was -17.0. running mean: -17.708611193962692\n",
      "ep361: game finished, reward: -1.0\n",
      "20.224083000000064\n",
      "resetting env. episode reward total was -17.0. running mean: -17.701525082023068\n",
      "ep362: game finished, reward: -1.0\n",
      "18.200184999999692\n",
      "resetting env. episode reward total was -19.0. running mean: -17.71450983120284\n",
      "ep363: game finished, reward: -1.0\n",
      "19.3238080000001\n",
      "resetting env. episode reward total was -19.0. running mean: -17.727364732890813\n",
      "ep364: game finished, reward: -1.0\n",
      "19.31973999999991\n",
      "resetting env. episode reward total was -17.0. running mean: -17.720091085561904\n",
      "ep365: game finished, reward: -1.0\n",
      "16.802324000000226\n",
      "resetting env. episode reward total was -20.0. running mean: -17.742890174706286\n",
      "ep366: game finished, reward: -1.0\n",
      "19.82393300000058\n",
      "resetting env. episode reward total was -19.0. running mean: -17.755461272959224\n",
      "ep367: game finished, reward: -1.0\n",
      "16.240993999999773\n",
      "resetting env. episode reward total was -21.0. running mean: -17.78790666022963\n",
      "ep368: game finished, reward: -1.0\n",
      "18.16575600000033\n",
      "resetting env. episode reward total was -17.0. running mean: -17.780027593627334\n",
      "ep369: game finished, reward: -1.0\n",
      "16.25248499999998\n",
      "resetting env. episode reward total was -18.0. running mean: -17.78222731769106\n",
      "ep370: game finished, reward: -1.0\n",
      "19.25573600000007\n",
      "resetting env. episode reward total was -17.0. running mean: -17.77440504451415\n",
      "ep371: game finished, reward: -1.0\n",
      "14.522391999999854\n",
      "resetting env. episode reward total was -20.0. running mean: -17.796660994069008\n",
      "ep372: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.58349399999952\n",
      "resetting env. episode reward total was -19.0. running mean: -17.80869438412832\n",
      "ep373: game finished, reward: -1.0\n",
      "15.727280999999493\n",
      "resetting env. episode reward total was -21.0. running mean: -17.840607440287037\n",
      "ep374: game finished, reward: -1.0\n",
      "23.075326000000132\n",
      "resetting env. episode reward total was -19.0. running mean: -17.852201365884167\n",
      "ep375: game finished, reward: -1.0\n",
      "17.36208600000009\n",
      "resetting env. episode reward total was -19.0. running mean: -17.863679352225326\n",
      "ep376: game finished, reward: -1.0\n",
      "18.358663999999408\n",
      "resetting env. episode reward total was -12.0. running mean: -17.80504255870307\n",
      "ep377: game finished, reward: -1.0\n",
      "17.241244000000734\n",
      "resetting env. episode reward total was -18.0. running mean: -17.80699213311604\n",
      "ep378: game finished, reward: -1.0\n",
      "19.405033000000003\n",
      "resetting env. episode reward total was -16.0. running mean: -17.78892221178488\n",
      "ep379: game finished, reward: -1.0\n",
      "18.802036000000044\n",
      "resetting env. episode reward total was -17.0. running mean: -17.781032989667032\n",
      "ep380: game finished, reward: -1.0\n",
      "18.111469000000398\n",
      "resetting env. episode reward total was -19.0. running mean: -17.793222659770365\n",
      "ep381: game finished, reward: -1.0\n",
      "20.10370199999943\n",
      "resetting env. episode reward total was -15.0. running mean: -17.76529043317266\n",
      "ep382: game finished, reward: -1.0\n",
      "16.815958999999566\n",
      "resetting env. episode reward total was -21.0. running mean: -17.797637528840934\n",
      "ep383: game finished, reward: -1.0\n",
      "17.124375000000327\n",
      "resetting env. episode reward total was -19.0. running mean: -17.809661153552526\n",
      "ep384: game finished, reward: -1.0\n",
      "20.08640699999978\n",
      "resetting env. episode reward total was -19.0. running mean: -17.821564542017\n",
      "ep385: game finished, reward: -1.0\n",
      "17.445128999999724\n",
      "resetting env. episode reward total was -20.0. running mean: -17.84334889659683\n",
      "ep386: game finished, reward: -1.0\n",
      "19.87990600000012\n",
      "resetting env. episode reward total was -19.0. running mean: -17.854915407630862\n",
      "ep387: game finished, reward: -1.0\n",
      "17.952162999999928\n",
      "resetting env. episode reward total was -18.0. running mean: -17.856366253554555\n",
      "ep388: game finished, reward: -1.0\n",
      "21.753396000000066\n",
      "resetting env. episode reward total was -14.0. running mean: -17.81780259101901\n",
      "ep389: game finished, reward: -1.0\n",
      "21.302549\n",
      "resetting env. episode reward total was -16.0. running mean: -17.79962456510882\n",
      "ep390: game finished, reward: -1.0\n",
      "21.24041600000055\n",
      "resetting env. episode reward total was -17.0. running mean: -17.791628319457736\n",
      "ep391: game finished, reward: -1.0\n",
      "17.0842990000001\n",
      "resetting env. episode reward total was -19.0. running mean: -17.80371203626316\n",
      "ep392: game finished, reward: -1.0\n",
      "23.59707200000048\n",
      "resetting env. episode reward total was -17.0. running mean: -17.79567491590053\n",
      "ep393: game finished, reward: -1.0\n",
      "22.49162299999989\n",
      "resetting env. episode reward total was -17.0. running mean: -17.787718166741527\n",
      "ep394: game finished, reward: -1.0\n",
      "18.992506000000503\n",
      "resetting env. episode reward total was -17.0. running mean: -17.779840985074113\n",
      "ep395: game finished, reward: -1.0\n",
      "19.981517999999596\n",
      "resetting env. episode reward total was -18.0. running mean: -17.78204257522337\n",
      "ep396: game finished, reward: -1.0\n",
      "21.003600000000006\n",
      "resetting env. episode reward total was -19.0. running mean: -17.79422214947114\n",
      "ep397: game finished, reward: -1.0\n",
      "19.765762999999424\n",
      "resetting env. episode reward total was -15.0. running mean: -17.766279927976427\n",
      "ep398: game finished, reward: -1.0\n",
      "19.01955799999996\n",
      "resetting env. episode reward total was -16.0. running mean: -17.74861712869666\n",
      "ep399: game finished, reward: -1.0\n",
      "19.75110399999994\n",
      "resetting env. episode reward total was -19.0. running mean: -17.761130957409698\n",
      "ep400: game finished, reward: -1.0\n",
      "21.752588999999716\n",
      "resetting env. episode reward total was -17.0. running mean: -17.753519647835603\n",
      "ep401: game finished, reward: -1.0\n",
      "19.37992899999972\n",
      "resetting env. episode reward total was -21.0. running mean: -17.785984451357248\n",
      "ep402: game finished, reward: -1.0\n",
      "23.583723000000646\n",
      "resetting env. episode reward total was -17.0. running mean: -17.778124606843676\n",
      "ep403: game finished, reward: -1.0\n",
      "20.035243000000264\n",
      "resetting env. episode reward total was -19.0. running mean: -17.79034336077524\n",
      "ep404: game finished, reward: -1.0\n",
      "27.712604000000283\n",
      "resetting env. episode reward total was -16.0. running mean: -17.772439927167486\n",
      "ep405: game finished, reward: -1.0\n",
      "24.675540000000183\n",
      "resetting env. episode reward total was -19.0. running mean: -17.784715527895813\n",
      "ep406: game finished, reward: -1.0\n",
      "23.584638999999697\n",
      "resetting env. episode reward total was -17.0. running mean: -17.776868372616857\n",
      "ep407: game finished, reward: -1.0\n",
      "21.621216999999888\n",
      "resetting env. episode reward total was -19.0. running mean: -17.78909968889069\n",
      "ep408: game finished, reward: -1.0\n",
      "26.415192999999817\n",
      "resetting env. episode reward total was -15.0. running mean: -17.761208692001784\n",
      "ep409: game finished, reward: -1.0\n",
      "14.939868000000388\n",
      "resetting env. episode reward total was -20.0. running mean: -17.783596605081765\n",
      "ep410: game finished, reward: -1.0\n",
      "17.072175999999672\n",
      "resetting env. episode reward total was -20.0. running mean: -17.805760639030947\n",
      "ep411: game finished, reward: -1.0\n",
      "19.799667999999656\n",
      "resetting env. episode reward total was -21.0. running mean: -17.837703032640636\n",
      "ep412: game finished, reward: -1.0\n",
      "18.71089099999972\n",
      "resetting env. episode reward total was -19.0. running mean: -17.849326002314232\n",
      "ep413: game finished, reward: -1.0\n",
      "20.235022999999273\n",
      "resetting env. episode reward total was -21.0. running mean: -17.88083274229109\n",
      "ep414: game finished, reward: -1.0\n",
      "18.176602999999886\n",
      "resetting env. episode reward total was -19.0. running mean: -17.89202441486818\n",
      "ep415: game finished, reward: -1.0\n",
      "17.583424999999806\n",
      "resetting env. episode reward total was -15.0. running mean: -17.8631041707195\n",
      "ep416: game finished, reward: -1.0\n",
      "18.64267700000073\n",
      "resetting env. episode reward total was -17.0. running mean: -17.854473129012305\n",
      "ep417: game finished, reward: -1.0\n",
      "20.1201800000008\n",
      "resetting env. episode reward total was -19.0. running mean: -17.865928397722183\n",
      "ep418: game finished, reward: -1.0\n",
      "14.301574999999502\n",
      "resetting env. episode reward total was -19.0. running mean: -17.877269113744962\n",
      "ep419: game finished, reward: -1.0\n",
      "22.366123999999218\n",
      "resetting env. episode reward total was -14.0. running mean: -17.838496422607513\n",
      "ep420: game finished, reward: -1.0\n",
      "17.279026999999587\n",
      "resetting env. episode reward total was -19.0. running mean: -17.850111458381438\n",
      "ep421: game finished, reward: -1.0\n",
      "14.977422999999362\n",
      "resetting env. episode reward total was -17.0. running mean: -17.841610343797626\n",
      "ep422: game finished, reward: -1.0\n",
      "17.159320000000662\n",
      "resetting env. episode reward total was -20.0. running mean: -17.86319424035965\n",
      "ep423: game finished, reward: -1.0\n",
      "21.80170200000066\n",
      "resetting env. episode reward total was -17.0. running mean: -17.854562297956054\n",
      "ep424: game finished, reward: -1.0\n",
      "17.824852000001556\n",
      "resetting env. episode reward total was -17.0. running mean: -17.846016674976497\n",
      "ep425: game finished, reward: -1.0\n",
      "20.097701000000598\n",
      "resetting env. episode reward total was -15.0. running mean: -17.81755650822673\n",
      "ep426: game finished, reward: -1.0\n",
      "20.345868000000337\n",
      "resetting env. episode reward total was -18.0. running mean: -17.819380943144463\n",
      "ep427: game finished, reward: -1.0\n",
      "21.789919000000737\n",
      "resetting env. episode reward total was -13.0. running mean: -17.771187133713017\n",
      "ep428: game finished, reward: -1.0\n",
      "17.665179999999964\n",
      "resetting env. episode reward total was -18.0. running mean: -17.773475262375886\n",
      "ep429: game finished, reward: -1.0\n",
      "17.17851100000007\n",
      "resetting env. episode reward total was -21.0. running mean: -17.805740509752127\n",
      "ep430: game finished, reward: -1.0\n",
      "24.157572999998592\n",
      "resetting env. episode reward total was -17.0. running mean: -17.797683104654606\n",
      "ep431: game finished, reward: -1.0\n",
      "16.889781000001676\n",
      "resetting env. episode reward total was -17.0. running mean: -17.78970627360806\n",
      "ep432: game finished, reward: -1.0\n",
      "16.100984000000608\n",
      "resetting env. episode reward total was -18.0. running mean: -17.791809210871982\n",
      "ep433: game finished, reward: -1.0\n",
      "16.814487999999983\n",
      "resetting env. episode reward total was -21.0. running mean: -17.823891118763264\n",
      "ep434: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.428812999998627\n",
      "resetting env. episode reward total was -18.0. running mean: -17.82565220757563\n",
      "ep435: game finished, reward: -1.0\n",
      "14.84969699999965\n",
      "resetting env. episode reward total was -16.0. running mean: -17.807395685499873\n",
      "ep436: game finished, reward: -1.0\n",
      "20.296304999999847\n",
      "resetting env. episode reward total was -19.0. running mean: -17.819321728644876\n",
      "ep437: game finished, reward: -1.0\n",
      "15.656087999999727\n",
      "resetting env. episode reward total was -20.0. running mean: -17.841128511358427\n",
      "ep438: game finished, reward: -1.0\n",
      "17.50057900000138\n",
      "resetting env. episode reward total was -17.0. running mean: -17.832717226244846\n",
      "ep439: game finished, reward: -1.0\n",
      "25.122754999998506\n",
      "resetting env. episode reward total was -18.0. running mean: -17.834390053982396\n",
      "ep440: game finished, reward: -1.0\n",
      "20.873718000000736\n",
      "resetting env. episode reward total was -19.0. running mean: -17.846046153442572\n",
      "ep441: game finished, reward: -1.0\n",
      "17.585934999999154\n",
      "resetting env. episode reward total was -18.0. running mean: -17.847585691908147\n",
      "ep442: game finished, reward: -1.0\n",
      "22.583349999998973\n",
      "resetting env. episode reward total was -16.0. running mean: -17.829109834989065\n",
      "ep443: game finished, reward: -1.0\n",
      "18.17975800000022\n",
      "resetting env. episode reward total was -21.0. running mean: -17.860818736639175\n",
      "ep444: game finished, reward: -1.0\n",
      "17.324650999998994\n",
      "resetting env. episode reward total was -21.0. running mean: -17.892210549272782\n",
      "ep445: game finished, reward: -1.0\n",
      "21.877628000000186\n",
      "resetting env. episode reward total was -21.0. running mean: -17.923288443780056\n",
      "ep446: game finished, reward: -1.0\n",
      "20.218182000000525\n",
      "resetting env. episode reward total was -19.0. running mean: -17.934055559342255\n",
      "ep447: game finished, reward: -1.0\n",
      "18.695402000001195\n",
      "resetting env. episode reward total was -17.0. running mean: -17.924715003748833\n",
      "ep448: game finished, reward: -1.0\n",
      "14.035498999999618\n",
      "resetting env. episode reward total was -18.0. running mean: -17.925467853711343\n",
      "ep449: game finished, reward: -1.0\n",
      "17.23505500000101\n",
      "resetting env. episode reward total was -21.0. running mean: -17.95621317517423\n",
      "ep450: game finished, reward: -1.0\n",
      "21.030236000000514\n",
      "resetting env. episode reward total was -18.0. running mean: -17.95665104342249\n",
      "ep451: game finished, reward: -1.0\n",
      "17.495010000000548\n",
      "resetting env. episode reward total was -19.0. running mean: -17.967084532988267\n",
      "ep452: game finished, reward: -1.0\n",
      "15.818513999998686\n",
      "resetting env. episode reward total was -21.0. running mean: -17.997413687658383\n",
      "ep453: game finished, reward: -1.0\n",
      "22.116063999999824\n",
      "resetting env. episode reward total was -18.0. running mean: -17.9974395507818\n",
      "ep454: game finished, reward: -1.0\n",
      "20.19002399999954\n",
      "resetting env. episode reward total was -18.0. running mean: -17.99746515527398\n",
      "ep455: game finished, reward: -1.0\n",
      "14.400840999998763\n",
      "resetting env. episode reward total was -18.0. running mean: -17.997490503721238\n",
      "ep456: game finished, reward: -1.0\n",
      "17.84851499999968\n",
      "resetting env. episode reward total was -20.0. running mean: -18.017515598684025\n",
      "ep457: game finished, reward: -1.0\n",
      "22.595728000000236\n",
      "resetting env. episode reward total was -12.0. running mean: -17.957340442697184\n",
      "ep458: game finished, reward: -1.0\n",
      "22.09344500000043\n",
      "resetting env. episode reward total was -10.0. running mean: -17.877767038270214\n",
      "ep459: game finished, reward: -1.0\n",
      "17.422888999999486\n",
      "resetting env. episode reward total was -17.0. running mean: -17.868989367887515\n",
      "ep460: game finished, reward: -1.0\n",
      "18.075161000000662\n",
      "resetting env. episode reward total was -18.0. running mean: -17.87029947420864\n",
      "ep461: game finished, reward: -1.0\n",
      "16.976300000000265\n",
      "resetting env. episode reward total was -17.0. running mean: -17.861596479466556\n",
      "ep462: game finished, reward: -1.0\n",
      "16.02587400000084\n",
      "resetting env. episode reward total was -20.0. running mean: -17.88298051467189\n",
      "ep463: game finished, reward: -1.0\n",
      "17.709703000000445\n",
      "resetting env. episode reward total was -19.0. running mean: -17.89415070952517\n",
      "ep464: game finished, reward: -1.0\n",
      "19.20846000000165\n",
      "resetting env. episode reward total was -18.0. running mean: -17.895209202429918\n",
      "ep465: game finished, reward: -1.0\n",
      "18.065980999999738\n",
      "resetting env. episode reward total was -21.0. running mean: -17.92625711040562\n",
      "ep466: game finished, reward: -1.0\n",
      "21.31850399999894\n",
      "resetting env. episode reward total was -18.0. running mean: -17.92699453930156\n",
      "ep467: game finished, reward: -1.0\n",
      "20.444019999999\n",
      "resetting env. episode reward total was -19.0. running mean: -17.937724593908545\n",
      "ep468: game finished, reward: -1.0\n",
      "27.003357999999935\n",
      "resetting env. episode reward total was -10.0. running mean: -17.85834734796946\n",
      "ep469: game finished, reward: -1.0\n",
      "18.747520000000804\n",
      "resetting env. episode reward total was -21.0. running mean: -17.889763874489766\n",
      "ep470: game finished, reward: -1.0\n",
      "19.768395999999484\n",
      "resetting env. episode reward total was -19.0. running mean: -17.90086623574487\n",
      "ep471: game finished, reward: -1.0\n",
      "22.97963499999969\n",
      "resetting env. episode reward total was -18.0. running mean: -17.90185757338742\n",
      "ep472: game finished, reward: -1.0\n",
      "20.0758850000002\n",
      "resetting env. episode reward total was -19.0. running mean: -17.912838997653548\n",
      "ep473: game finished, reward: -1.0\n",
      "17.602906000000075\n",
      "resetting env. episode reward total was -17.0. running mean: -17.903710607677013\n",
      "ep474: game finished, reward: -1.0\n",
      "17.454490000000078\n",
      "resetting env. episode reward total was -19.0. running mean: -17.914673501600245\n",
      "ep475: game finished, reward: -1.0\n",
      "20.89713599999959\n",
      "resetting env. episode reward total was -19.0. running mean: -17.925526766584245\n",
      "ep476: game finished, reward: -1.0\n",
      "20.390159000000494\n",
      "resetting env. episode reward total was -17.0. running mean: -17.916271498918405\n",
      "ep477: game finished, reward: -1.0\n",
      "23.685835000000225\n",
      "resetting env. episode reward total was -21.0. running mean: -17.94710878392922\n",
      "ep478: game finished, reward: -1.0\n",
      "18.55967899999996\n",
      "resetting env. episode reward total was -20.0. running mean: -17.967637696089927\n",
      "ep479: game finished, reward: -1.0\n",
      "20.703013000000283\n",
      "resetting env. episode reward total was -17.0. running mean: -17.95796131912903\n",
      "ep480: game finished, reward: -1.0\n",
      "20.2029849999999\n",
      "resetting env. episode reward total was -16.0. running mean: -17.93838170593774\n",
      "ep481: game finished, reward: -1.0\n",
      "19.67924099999982\n",
      "resetting env. episode reward total was -20.0. running mean: -17.95899788887836\n",
      "ep482: game finished, reward: -1.0\n",
      "15.86142700000164\n",
      "resetting env. episode reward total was -21.0. running mean: -17.989407909989577\n",
      "ep483: game finished, reward: -1.0\n",
      "21.17561299999943\n",
      "resetting env. episode reward total was -18.0. running mean: -17.98951383088968\n",
      "ep484: game finished, reward: -1.0\n",
      "22.594763000000967\n",
      "resetting env. episode reward total was -12.0. running mean: -17.929618692580785\n",
      "ep485: game finished, reward: -1.0\n",
      "16.684504999999263\n",
      "resetting env. episode reward total was -18.0. running mean: -17.930322505654978\n",
      "ep486: game finished, reward: -1.0\n",
      "17.62664699999914\n",
      "resetting env. episode reward total was -15.0. running mean: -17.901019280598426\n",
      "ep487: game finished, reward: -1.0\n",
      "23.840081000000282\n",
      "resetting env. episode reward total was -14.0. running mean: -17.86200908779244\n",
      "ep488: game finished, reward: -1.0\n",
      "17.845248000001448\n",
      "resetting env. episode reward total was -18.0. running mean: -17.863388996914516\n",
      "ep489: game finished, reward: -1.0\n",
      "19.07723299999998\n",
      "resetting env. episode reward total was -21.0. running mean: -17.89475510694537\n",
      "ep490: game finished, reward: -1.0\n",
      "21.307551999998395\n",
      "resetting env. episode reward total was -17.0. running mean: -17.88580755587592\n",
      "ep491: game finished, reward: -1.0\n",
      "18.272069000000556\n",
      "resetting env. episode reward total was -16.0. running mean: -17.86694948031716\n",
      "ep492: game finished, reward: -1.0\n",
      "20.61707100000058\n",
      "resetting env. episode reward total was -15.0. running mean: -17.83827998551399\n",
      "ep493: game finished, reward: -1.0\n",
      "22.144859999998516\n",
      "resetting env. episode reward total was -17.0. running mean: -17.82989718565885\n",
      "ep494: game finished, reward: -1.0\n",
      "24.816646999999648\n",
      "resetting env. episode reward total was -13.0. running mean: -17.781598213802262\n",
      "ep495: game finished, reward: -1.0\n",
      "23.37691199999972\n",
      "resetting env. episode reward total was -21.0. running mean: -17.81378223166424\n",
      "ep496: game finished, reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.7258499999989\n",
      "resetting env. episode reward total was -19.0. running mean: -17.8256444093476\n",
      "ep497: game finished, reward: -1.0\n",
      "19.001162000000477\n",
      "resetting env. episode reward total was -21.0. running mean: -17.857387965254123\n",
      "ep498: game finished, reward: -1.0\n",
      "18.542186999999103\n",
      "resetting env. episode reward total was -19.0. running mean: -17.86881408560158\n",
      "ep499: game finished, reward: -1.0\n",
      "21.83866400000079\n",
      "resetting env. episode reward total was -17.0. running mean: -17.860125944745565\n",
      "ep500: game finished, reward: -1.0\n",
      "25.48303499999929\n",
      "resetting env. episode reward total was -13.0. running mean: -17.81152468529811\n",
      "ep501: game finished, reward: -1.0\n",
      "22.522923000000446\n",
      "resetting env. episode reward total was -15.0. running mean: -17.783409438445126\n",
      "ep502: game finished, reward: -1.0\n",
      "24.630150999999387\n",
      "resetting env. episode reward total was -17.0. running mean: -17.775575344060677\n",
      "ep503: game finished, reward: -1.0\n",
      "19.253445000000283\n",
      "resetting env. episode reward total was -21.0. running mean: -17.80781959062007\n",
      "ep504: game finished, reward: -1.0\n",
      "22.201184000001376\n",
      "resetting env. episode reward total was -17.0. running mean: -17.79974139471387\n",
      "ep505: game finished, reward: -1.0\n",
      "26.830987999999707\n",
      "resetting env. episode reward total was -17.0. running mean: -17.791743980766732\n",
      "ep506: game finished, reward: -1.0\n",
      "21.77452799999992\n",
      "resetting env. episode reward total was -17.0. running mean: -17.783826540959065\n",
      "ep507: game finished, reward: -1.0\n",
      "16.84281899999951\n",
      "resetting env. episode reward total was -18.0. running mean: -17.785988275549474\n",
      "ep508: game finished, reward: -1.0\n",
      "17.67566099999931\n",
      "resetting env. episode reward total was -17.0. running mean: -17.77812839279398\n",
      "ep509: game finished, reward: -1.0\n",
      "19.908659999999145\n",
      "resetting env. episode reward total was -14.0. running mean: -17.740347108866043\n",
      "ep510: game finished, reward: -1.0\n",
      "20.622889999998733\n",
      "resetting env. episode reward total was -15.0. running mean: -17.712943637777382\n",
      "ep511: game finished, reward: -1.0\n",
      "18.22830200000135\n",
      "resetting env. episode reward total was -15.0. running mean: -17.685814201399605\n",
      "ep512: game finished, reward: -1.0\n",
      "18.463353000001007\n",
      "resetting env. episode reward total was -19.0. running mean: -17.698956059385612\n",
      "ep513: game finished, reward: -1.0\n",
      "18.721878000000288\n",
      "resetting env. episode reward total was -21.0. running mean: -17.731966498791756\n",
      "ep514: game finished, reward: -1.0\n",
      "15.320537999999942\n",
      "resetting env. episode reward total was -19.0. running mean: -17.74464683380384\n",
      "ep515: game finished, reward: -1.0\n",
      "20.01855500000056\n",
      "resetting env. episode reward total was -14.0. running mean: -17.707200365465802\n",
      "ep516: game finished, reward: -1.0\n",
      "19.520495000000665\n",
      "resetting env. episode reward total was -15.0. running mean: -17.680128361811143\n",
      "ep517: game finished, reward: -1.0\n",
      "17.998507000000245\n",
      "resetting env. episode reward total was -19.0. running mean: -17.693327078193033\n",
      "ep518: game finished, reward: -1.0\n",
      "19.325473999999303\n",
      "resetting env. episode reward total was -17.0. running mean: -17.686393807411104\n",
      "ep519: game finished, reward: -1.0\n",
      "17.42041000000063\n",
      "resetting env. episode reward total was -21.0. running mean: -17.719529869336995\n",
      "ep520: game finished, reward: -1.0\n",
      "30.708505999999034\n",
      "resetting env. episode reward total was -13.0. running mean: -17.672334570643624\n",
      "ep521: game finished, reward: -1.0\n",
      "27.052262999999584\n",
      "resetting env. episode reward total was -14.0. running mean: -17.63561122493719\n",
      "ep522: game finished, reward: -1.0\n",
      "11.806109999999535\n",
      "resetting env. episode reward total was -21.0. running mean: -17.669255112687818\n",
      "ep523: game finished, reward: -1.0\n",
      "21.98497900000075\n",
      "resetting env. episode reward total was -21.0. running mean: -17.702562561560942\n",
      "ep524: game finished, reward: -1.0\n",
      "22.581034999999247\n",
      "resetting env. episode reward total was -18.0. running mean: -17.705536935945332\n",
      "ep525: game finished, reward: -1.0\n",
      "19.22613899999851\n",
      "resetting env. episode reward total was -20.0. running mean: -17.728481566585877\n",
      "ep526: game finished, reward: -1.0\n",
      "20.933728999998493\n",
      "resetting env. episode reward total was -20.0. running mean: -17.751196750920016\n",
      "ep527: game finished, reward: -1.0\n",
      "19.715824000000794\n",
      "resetting env. episode reward total was -13.0. running mean: -17.703684783410814\n",
      "ep528: game finished, reward: -1.0\n",
      "20.381899999998495\n",
      "resetting env. episode reward total was -16.0. running mean: -17.686647935576705\n",
      "ep529: game finished, reward: -1.0\n",
      "16.68365199999971\n",
      "resetting env. episode reward total was -21.0. running mean: -17.71978145622094\n",
      "ep530: game finished, reward: -1.0\n",
      "28.769102999998722\n",
      "resetting env. episode reward total was -9.0. running mean: -17.63258364165873\n",
      "ep531: game finished, reward: -1.0\n",
      "12.762328999999227\n",
      "resetting env. episode reward total was -19.0. running mean: -17.646257805242143\n",
      "ep532: game finished, reward: -1.0\n",
      "19.384460999999646\n",
      "resetting env. episode reward total was -19.0. running mean: -17.659795227189722\n",
      "ep533: game finished, reward: -1.0\n",
      "21.221309000000474\n",
      "resetting env. episode reward total was -20.0. running mean: -17.683197274917823\n",
      "ep534: game finished, reward: -1.0\n",
      "21.958716999999524\n",
      "resetting env. episode reward total was -17.0. running mean: -17.676365302168648\n",
      "ep535: game finished, reward: -1.0\n",
      "22.916709000000992\n",
      "resetting env. episode reward total was -17.0. running mean: -17.669601649146962\n",
      "ep536: game finished, reward: -1.0\n",
      "20.236242000000857\n",
      "resetting env. episode reward total was -19.0. running mean: -17.682905632655494\n",
      "ep537: game finished, reward: -1.0\n",
      "18.35912500000086\n",
      "resetting env. episode reward total was -18.0. running mean: -17.68607657632894\n",
      "ep538: game finished, reward: -1.0\n",
      "16.86393600000156\n",
      "resetting env. episode reward total was -21.0. running mean: -17.71921581056565\n",
      "ep539: game finished, reward: -1.0\n",
      "16.785830000000715\n",
      "resetting env. episode reward total was -19.0. running mean: -17.732023652459993\n",
      "ep540: game finished, reward: -1.0\n",
      "17.160754000000452\n",
      "resetting env. episode reward total was -19.0. running mean: -17.744703415935394\n",
      "ep541: game finished, reward: -1.0\n",
      "20.906669999998485\n",
      "resetting env. episode reward total was -18.0. running mean: -17.74725638177604\n",
      "ep542: game finished, reward: -1.0\n",
      "18.570364000001064\n",
      "resetting env. episode reward total was -20.0. running mean: -17.769783817958277\n",
      "ep543: game finished, reward: -1.0\n",
      "18.159598000000187\n",
      "resetting env. episode reward total was -19.0. running mean: -17.782085979778696\n",
      "ep544: game finished, reward: -1.0\n",
      "23.005358000000342\n",
      "resetting env. episode reward total was -13.0. running mean: -17.734265119980908\n",
      "ep545: game finished, reward: -1.0\n",
      "20.962869999999384\n",
      "resetting env. episode reward total was -21.0. running mean: -17.7669224687811\n",
      "ep546: game finished, reward: -1.0\n",
      "21.43847000000096\n",
      "resetting env. episode reward total was -18.0. running mean: -17.76925324409329\n",
      "ep547: game finished, reward: -1.0\n",
      "22.896670000000086\n",
      "resetting env. episode reward total was -18.0. running mean: -17.771560711652356\n",
      "ep548: game finished, reward: -1.0\n",
      "24.209124999999403\n",
      "resetting env. episode reward total was -16.0. running mean: -17.75384510453583\n",
      "ep549: game finished, reward: -1.0\n",
      "23.402969999999186\n",
      "resetting env. episode reward total was -15.0. running mean: -17.72630665349047\n",
      "ep550: game finished, reward: -1.0\n",
      "21.052615000000515\n",
      "resetting env. episode reward total was -14.0. running mean: -17.689043586955567\n",
      "ep551: game finished, reward: -1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-021ee83511b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;31m# forward the policy network and sample an action from the returned probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0maprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m# roll the dice!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-021ee83511b8>\u001b[0m in \u001b[0;36mpolicy_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# ReLU nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save2.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    try: print(time.clock() - start)\n",
    "    except: print(\"boo\")\n",
    "    print('resetting env. episode reward total was {}. running mean: {}'.format(reward_sum, running_reward))\n",
    "    start = time.clock()\n",
    "    if episode_number % 50 == 0: pickle.dump(model, open('save2.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print(('ep{}: game finished, reward: {}'.format(episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
